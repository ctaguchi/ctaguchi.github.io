<!DOCTYPE html>
<html lang="ja">
<head>
    <meta charset="UTF-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <script type="text/javascript" async 
        src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-MML-AM_CHTML">
    </script>
    <title>Chapter 3</title>
</head>
<body>
    <h1>3. 線形ニューラルネットワーク</h1>
    <div>
        深層ニューラルネットワークの詳細に入る前に、まずはニューラルネットワークの訓練の基礎をさらっておこう。この章では、単純なニューラルネットワークの構造の定式化、データの扱い、損失関数の選択、モデルの訓練といった、訓練過程の全体を概観する。わかりやすくするために、まずは最も単純な概念から始めよう。運よく、線形回帰やソフトマックス回帰といった古典的な統計的学習技術は、線形ニューラルネットワークに落とし込むことができる。これらの古典的なアルゴリズムから始めることで、本書で後ほど扱われる複雑な技術を理解するための基礎概念を導入する。
    <!--
    Before we get into the details of deep neural networks,
    we need to cover the basics of neural network training.
    In this chapter, we will cover the entire training process,
    including defining simple neural network architectures,
    handling data, specifying a loss function, and training the model.
    In order to make things easier to grasp,
    we begin with the simplest concepts.
    Fortunately, classic statistical learning techniques
    such as linear and softmax regression can be cast as linear neural networks.
    Starting from these classic algorithms,
    we will introduce you to the basics,
    providing the basis for more complex techniques in the rest of the book.
    -->
    </div>
    <h2>3.1. 線形回帰</h2>
    <div>
    「回帰」とは、一つ以上の独立変数と、一つの従属変数の間の関係をモデル化する手法の総称である。自然科学や社会科学では、回帰は通常入力と出力の間の関係を特徴付けるために用いられるが。他方で、機械学習では、回帰はもっぱら「予測」に用いられる。
    </div>
    <div>
    回帰問題は、数値予測に常につきまとう。よくある例としては、家や株の価格を予測したり、患者の入院日数を予測したり、衣服の販売における需要予測を行ったりといったように、無数に挙げられる。全ての予測問題が古典的な回帰問題というわけではない。これからの章では、どのカテゴリに属するかということを予測する、分類問題を紹介する。
    </div>

    <h3>3.1.1 線形回帰の基本事項</h3>
    <div>
        線形回帰は、回帰に用いられる標準的な手法としては最も単純かつ人気のある手法である。線形回帰は19世紀初頭に、いくつかの単純な前提に端を発している。まず、一つ以上の独立変数 \(\textbf{x}\) と従属変数 \(y\) の関係は、線形であると仮定する。つまり、 \(y\) は、観測上のノイズを考慮して、重みをつけた \(\textbf{x}\) の和として表現できるとする。第二に、あらゆるノイズはガウス分布に従うものとする。
    </div>
    <div>
        手法に慣れるために、早速例を見てみよう。今、我々は家を探しているとしよう。家の広さ（平米）と築年数に基づいて、家の価格（ドル）を推定したい。家の価格を予測するモデルを実際に構築するためには、一つ一つの家の価格、広さ、築年数のデータを含む売り上げのデータセットを手に入れる必要がある。機械学習の用語では、このようなデータセットは訓練データセット training dataset または訓練セット training set と呼ばれ、各行（一つの物件の売り上げに対応するデータ）は標本 example, sample　（あるいはデータ点 data point 、データ・インスタンス data instance）と呼ばれる。我々が今予測しようとしているもの（価格）は、ラベル label （あるいはターゲット target ）と呼ばれる。予測が依存している独立変数（築年数と広さ）は、特徴 features （あるいは共変量 covariates ）と呼ばれる。
    </div>
    <div>
        典型的には、データセットにおける標本数として \(n\) を用いる。各標本の番号を \(i\) で表すと、それぞれの入力は \(\textbf{x} = [ x_{1}^{(i)}, x_{2}^{(i)} ]^{\mathrm{T}} \) 、対応するラベルを \( y^{(i)} \) と表せる。
    </div>
    <h4>3.1.1.1 線形モデル</h4>
    <div>
        線形の条件は、ターゲット（価格）が特徴量（広さと築年数）の重み付き和で表すことができる、ということであった。
    </div>
    <div>
        \begin{align}
        \mathrm{price} = w_{\mathrm{area}} \cdot \mathrm{area} + w_{\mathrm{age}} \cdot \mathrm{age} + b.
        \tag{3.1.1}
        \label{eq:price}
        \end{align}
    </div>
    <div>
        式 \eqref{eq:price} において、 \(w_{\mathrm{area}}\) と \(w_{\mathrm{age}}\) は重み weights と呼ばれ、\(b\) はバイアス bias （あるいはオフセット offset 、切片 intercept とも）と呼ばれる。重みは、予測においてそれぞれの特徴量がどれくらい影響を及ぼしているのかということを決定づける。バイアスは、全ての特徴量がゼロであるようなときに、予測される価格がどのような値をとるのか示しているだけである。実際には、面積がゼロの家や築年数がきっぱりゼロちょうどであるような家を見かけることなどないが、モデルの表現力を向上させるためにこのバイアスは必要なものとなる。厳密に言えば、 \eqref{eq:price} は、重み付き和による特徴量とバイアスの付加を組み合わせた線形変換であるから、入力の特徴量のアフィン変換であると言える。
    </div>
    <div>
        データセットが与えられたなら、我々の目標は、モデルによる予測数値が、データ内で観測される実際の数値と比較した時に、平均的に最もできるだけ正確になるように重み \(\textbf{w}\) とバイアス \(b\) を選ぶことである。入力の特徴量のアフィン変換によって出力の予測が決定づけられるモデルは、アフィン変換が選択された重みとバイアスで決定されるような線形モデルである。
    </div>
    <div>
        数個の特徴量しか含まないデータセットを扱うのが一般的であるような分野では、式 \eqref{eq:price} のように長大に書き下すこともあるだろう。しかし、機械学習では、多次元のデータセットを扱うのが普通であるため、線形代数の表記を用いる方が便利である。扱う入力が \(d\) 個の特徴量を持っているとき、予測する値 \(\hat{y}\) （\(y\) の上の「ハット」は推定を表す）は、次のように表せる。
    </div>
    <div>
        \begin{align}
            \hat{y} = w_1 x_1 + \dots + w_d x_d + b.
            \tag{3.1.2}
            \label{eq:wx}
        \end{align}
    </div>
    <div>
        全ての特徴量を \(d\) 次元のベクトル \(\textbf{x} \in \mathbb{R}^d\) に、また全ての重みをベクトル \(\textbf{w} \in \mathbb{R}^d\) に格納すると、式 \eqref{eq:wx} は次のように簡潔に書き換えることができる。
    </div>
    <div>
        \begin{align}
            \hat{y} = \textbf{w}^\mathrm{T} \textbf{x} + b
            \tag{3.1.3}
            \label{eq:wxlinear}
        \end{align}
    </div>
    <div>
        式 \eqref{eq:wxlinear} において、ベクトル \(\textbf{x}\) は、単一の標本の持つ一連の特徴量をさす。データセットの \(n\) 個全ての標本の特徴量に言及したいときは、計画行列 \(\textbf{X} \in \mathbb{R}^{n \times d}\) を用いるのが便利である。ここで、\(\textbf{x}\) は、各行に標本、各列に特徴量を有する行列である。
    </div>
    <div>
        全ての特徴量を有する \(\textbf{X}\) について、それに対応する予測 \(\hat{y} \in \mathbb{R}^n\) は、行列とベクトルの積で表すことができる。
    </div>
    <div>
        \begin{align}
            \hat{\textbf{y}} = \textbf{Xw} + b
            \tag{3.1.4}
            \label{eq:xwmatrix}
        \end{align}
    </div>
    <div>
        なお、バイアスを足すときにブロードキャストが行われている。訓練データセット \(\textbf{X}\) と、それに対応する既知のラベル \(y\) が与えられているとき、線形回帰の目標は、\(\textbf{X}\) と同様の分布から抽出した新しい標本に対して予測されるラベルの誤差が最も小さくなるような \(\textbf{w}\) と \(b\) を見つけることである。
    </div>
    <div>
        たとえ \(\textbf{x}\) が与えられたときに \(y\) を予測する最良のモデルが線形だとしても、\(n\) 個の標本から成る現実世界のデータセットの全ての標本 \(1 \leqq i \leqq n\) について \(\textbf{w}^{\mathrm{T}} \textbf{x}(i) + b\) が \(\textbf{y}(i)\) と一致するようなケースはないだろう。例えば、特徴量 \(\textbf{X}\) やラベル　\(y\) を観測するときに用いる手法によっては、多かれ少なかれ誤差が生じうる。したがって、根底にある関係が線形であることが確実だとしても、そのような誤差を説明するためにノイズの項を含める必要がある。
    </div>
    <div>
        最良のパラメータ \(\textbf{w}\) と \(b\) （あるいはモデル・パラメータ model parameters）を探す前に、以下の二つのことを確認しておこう。一つは、モデルが得られた時の評価手法で、もう一つはそのモデルの質を高めるための更新のプロセスである。
    </div>
    <h4>3.1.1.2. 損失関数</h4>
    <h4>3.1.1.4. 解析的解法</h4>
    <h4>3.1.1.5. ミニバッチ確率的勾配降下法</h4>
    <h4>3.1.1.5. 学習されたモデルで予測する</h4>
    <h3>3.1.2. 高速化のためのベクトル化</h3>
    <h3>3.1.3. 正規分布と二乗誤差</h3>
    <h3>3.1.4. 線形回帰から深層ネットワークへ</h3>
    <h4>3.1.4.1. ニューラルネットワークの図解</h4>
    <h4>3.1.4.2. 生物学</h4>
    <h3>3.1.5. まとめ</h3>
    <h2>3.2. 線形回帰を一から実装する</h2>
    <h3>3.2.1. データセットの生成</h3>
    <h3>3.2.2. データセットの読み込み</h3>
    <h3>3.2.3. モデル・パラメータの初期化</h3>
    <h3>3.2.4. モデルの決定</h3>
    <h3>3.2.5. 損失関数の決定</h3>
    <h3>3.2.6. 最適化アルゴリズムの決定</h3>
    <h3>3.2.7. 訓練</h3>
    <h3>3.2.8. まとめ</h3>
    <h2>3.3. 線形回帰の簡単な実装</h2>
    <h2>3.4. ソフトマックス回帰</h2>
    <h3>3.4.1. 分類問題</h3>
    <h3>3.4.2. ネットワークの構造</h3>
    <h3>3.4.3. 全連結層のパラメータ化のコスト</h3>
    <h3>3.4.4. ソフトマックスによる演算</h3>
    <h3>3.4.5. ミニバッチのベクトル化</h3>
    <h3>3.4.6. 損失関数</h3>
    <h4>3.4.6.1. 対数尤度</h4>
    <h4>3.4.6.2. ソフトマックスと導関数</h4>
    <h4>3.4.6.3. 交差エントロピー誤差</h4>
    <h3>3.4.7. 情報理論の基礎</h3>
    <h4>3.4.7.1. エントロピー</h4>
    <h4>3.4.7.2. サプライザル</h4>
    <h4>3.4.7.3. 交差エントロピー（もう一度）</h4>
    <h3>3.4.8. モデルの予測と評価</h3>
    <h3>3.4.9. まとめ</h3>
    <h2>3.5. 画像分類データセット</h2>
    <h3>3.5.1. データセットの読み込み</h3>
    <h3>3.5.2. ミニバッチの読み込み</h3>
    <h3>3.5.3. 組み合わせると......</h3>
    <h3>3.5.4. まとめ</h3>
    <h2>3.6. ソフトマックス回帰を一から実装する</h2>
    <h3>3.6.1. モデル・パラメータの初期化</h3>
    <h3>3.6.2. ソフトマックスの演算の定義</h3>
    <h3>3.6.3. モデルの決定</h3>
    <h3>3.6.4. 損失関数の決定</h3>
    <h3>3.6.5. 分類の精度</h3>
    <h3>3.6.6. 訓練</h3>
    <h3>3.6.7. 推定</h3>
    <h3>3.6.8. まとめ</h3>
    <h2>3.7. ソフトマックス回帰の簡単な実装</h2>
    <h3>3.7.1. モデル・パラメータの初期化</h3>
    <h3>3.7.2. ソフトマックス回帰（もう一度）</h3>
    <h3>3.7.3. 最適化アルゴリズム</h3>
    <h3>3.7.4. 訓練</h3>
    <h3>3.7.5. まとめ</h3>
</body>
</html>