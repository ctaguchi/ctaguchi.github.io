<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <script type="text/javascript" async 
    src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-MML-AM_CHTML">
    </script>
    <title>Chapter 4</title>
</head>
<body>
    <h1>4. 多層パーセプトロン</h1>
    <div>
        この章では、ついに真の意味での深層ネットワークを導入することになる。最も単純な深層ネットワークは多層パーセプトロンと呼ばれ、ニューロンから構成される複数の層から成っている。ある層のそれぞれのニューロンは前の層と次の層のニューロンと完全に連結しており、前の層から受け取った入力を出力として次の層に受け渡している。さて、高容量のモデルを訓練するときには、過学習の危険性がある。したがって、この章では、過学習、過少学習、モデル選択の概念について取り上げる。これらの問題に取り組むために、まずは重み減衰 weight decay やドロップアウト dropout といった標準化のテクニックを学ぶ。また、深層ネットワークの学習を成功させるための鍵となる、数値的安定性やパラメータの初期化に関する議論も行う。この章を通して、概念をしっかり把握することでなく、深層ネットワークを用いた実践も行う。そして、章末では、最初に取り上げた家賃の予測といった現実問題に適用する。計算的なパフォーマンスや、スケーラビリティ、モデルの効率性については今後の章で触れることになる。
    </div>
    <h2>4.1. 多層パーセプトロン</h2>
    <div>
        第三章では、ソフトマックス回帰を導入し、アルゴリズムを一から実装したり高度なAPIを用いて実装して、低解像度の画像から服飾の十個のカテゴリーに分類する分類器を訓練した。また、その中で、データの操作方法や、出力を有効な確率分布に落とし込み、適切な損失関数を適用し、モデルのパラメータに関して損失を最小化する手法を学んできた。さて、単純な線形モデルにおけるこれらの手法を学んできたので、比較的豊かな一連のモデルであり本書の主題でもある、深層ニューラル・ネットワークへの冒険をはじめよう。
    </div>
    <h3>4.1.1. 隠れ層</h3>
    3.1.1.1.章では、バイアス項を含んだ線形変換である、アフィン変換について説明した。ここで、図3.4.1.に示したような、ソフトマックス回帰の例のモデルの構造を思い出してみよう。このモデルでは、一つのアフィン変換を行った後にソフトマックスによる演算を行うことで、入力から出力へ直接写像している。もし正解ラベルが本当に入力データとアフィン変換によって結びついているならば、この手法は有効であろう。しかし、アフィン変換における線形性は、強い仮定を前提としている。
    <h4>4.1.1.1. 線形モデルが誤る可能性</h4>
    <div>
    例えば、線形性は、弱い意味での単調性を前提としている。つまり、特徴量の増加に伴って、モデルの出力は常に（対応する重みが正の時に）増加するか、（対応する重みが負の時に）減少するかのどちらかである。場合を限定すれば、これは理にかなっていると言える。例えば、ある個人がローンを返済するかどうかを予測したい時には、収入以外の全ての条件を等しいと仮定すれば、収入が多い人の方が、低収入の人よりもほど返済する見込みがあると想像がつく。単調的ではあるが、この関係性はおそらく、返済の確率とは線形的に結びついていないと思われる。例えば、0ドルから5万ドルに収入が増加した時は、100万ドルから105万ドルに収入が増加した場合よりも返済する確率が大幅に上昇するだろう。このような場合に対処する一つの手段としては、特徴量として収入の対数を取ったものを用いるなどして、線形性をよりそれらしく担保するようなデータの前処理が挙げられるだろう。
    </div>
    <div>
        もちろん、単調性を持たない例は容易に考えつく。例えば、体温に基づいて死の確率を予測したいとする。体温が 37°C (98.6°F) 以上の人は、体温が高いほど死の危険性があることを示している。しかし、37°C 以下の人に関していえば、体温が高いほど死の危険性が高まるとすれば、どうすれば良いのだろうか。この場合も、一捻り加えた前処理をすることで問題を解決できる。つまり、37°Cからの距離を特徴量とすれば良いのである。
    </div>
    <div>
        では、猫と犬の画像の分類にあたっては、どのようにすれば良いだろうか。座標 (13, 17) のピクセルの強度が上がると、その画像が犬を表している可能性（尤度）が上がったり下がったりするのだろうか。線形モデルに頼るということは、それぞれのピクセルの明るさの評価をすることによってのみ猫と犬の区別をすることができる、という暗黙の仮定を含んでいる。しかし、現実において色を真逆に置き換えても人が認識するカテゴリーは変わらないのならば、この手法の限界は明らかである。
    </div>
    <div>
        このような例では、以前に見た例と比べると、一見したところ線形性に限界があるように思えるが、実は単純な前処理によってこの問題に取り組むことができると考えられる。なぜなら、ピクセルの重要性は周辺環境（周辺のピクセルの値）に依存するからである。このように各特徴量の周辺との相互作用を考慮したデータの表現であれば線形モデルが適切だと考えられるが、それを実際にどのように手計算すべきかは不明である。深層ニューラル・ネットワークでは、観察データを用いて、隠れ層を介した表現と、その表現に影響を与える線形推定器を同時に学習する。
    </div>
    

    <h4>4.1.1.2. 隠れ層の統合</h4>
    <div>
        隠れ層を統合することで、線形モデルの限界を克服し、さらに広い範囲の関数を扱うことができるようになる。最も単純に隠れ層を統合するには、互いに完全に連結した層を積み重ねることが考えられる。各層は、出力が得られるまでその上の層に値を渡す。最初の \(L-1\) 個の層を表現と考えると、最終層は線形推定器であると考えることができる。この構造は一般的に多層パーセプトロンと呼ばれ、よく MLP と略されている。下に MLP の構造を図示する (図4.1.1）。
    </div>
    <div>
        この MLP は4つの入力、3つの出力、そして5つの隠れユニットから成る隠れ層によって構成されている。入力層では計算処理はないため、このネットワークで出力を得るためには隠れ層と出力層の計算を実装する必要がある。したがって、この MLP における層の数は2である。なお、これらの層は互いに完全に連結していることに注意されたい。入力は全て隠れ層の全てのニューロンに与えられ、今度はこれらのニューロンから出力の全てのニューロンに値が渡される。しかしながら、3.4.3章で触れたように、完全連結層を持った MLP のパラメータ化コストはとてつもなく高く、入力と出力のサイズを変えなくとも、パラメータの保持とモデルの効率性の間でトレード・オフになってしまう可能性がある [Zhang et al., 2021]。
    </div>
    <h4>4.1.1.3. 線形モデルから非線形モデルへ</h4>
    <div>
        前回同様、それぞれ \(d\) 個の入力（特徴量）を持った \(n\) 個のサンプルから成るミニバッチを、行列 \(\textbf{X} \in \mathbb{R}^{n \times d}\) と表す。\(h\) 個の隠れユニットから成る1層の隠れ層を持った MLP に関しては、隠れ層の出力（<b>隠れ表現</b>）を \(\textbf{H} \in \mathbb{R}^{n \times h}\) で表す。数学やコードでは、この \(\textbf{H}\) は<b>隠れ層変数</b>や<b>隠れ変数</b>とも呼ばれる。隠れ層と出力層は完全に連結しているので、隠れ層の重みは \( \textbf{W}^{(1)} \in \mathbb{R}^{d \times \h}) 、バイアスは \( \textbf{b} \in \mathbb{R}^{1 \times h} \) 、そして出力層の重みは \( \textbf{X}^{(2)} \in \mathbb{R}^{h \times q}\)、出力層のバイアスは \( b^{(2)} \in \mathbb{R}^{1 \times q}\) と表すことができる。形式的には、一層の隠れ層から成る MLP の出力 \( \textbf{O} \in \mathbb{R}^{n \times q}\) は、次のように書ける。
    </div>
    <div>
        \( \textbf{HO} = \textbf{XW} + \textbf{b}^{(1)}\), 
        𝐇𝐎=𝐗𝐖(1)+𝐛(1),=𝐇𝐖(2)+𝐛(2).
    </div>
    <h4> 4.1.1.4. 普遍近似</h4>
    <h3>4.1.2. 活性化関数</h3>
    <h4>4.1.2.1. ReLU関数</h4>
    <h4>4.1.2.2. シグモイド関数</h4>
    <h4>4.1.2.3. 双曲線正接関数 tanh</h4>
    <h3>4.1.3. まとめ</h3>
</body>
</html>